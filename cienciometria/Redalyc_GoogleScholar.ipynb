{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uu-sf4Ukvbb-"
   },
   "source": [
    "# Google Scholar \n",
    "By using lookup fields:\n",
    "\n",
    "<a href=\"http://bit.ly/gslookup\">https://scholar.google.co.in/scholar_lookup?<font color=\"red\">title=</font>Estimates+for+the+number+of+sums+and+products+and+for+exponential+sums+in+fields+of+prime+order&<font color=\"red\">author=</font>Jean+Bourgain&author=AA+Glibichuk&<font color=\"red\">author=</font>SERGEI+VLADIMIROVICH+Konyagin&<font color=\"red\">year=</font>2006&<font color=\"red\">doi=</font>10.1112/S0024610706022721&<font color=\"red\">publisher=</font>Oxford+University+Press&<font color=\"red\">journal=</font>Journal+of+the+London+Mathematical+Society&<font color=\"red\">volume=</font>73&<font color=\"red\">issue=</font>2&<font color=\"red\">pages=</font>380-398&hl=en</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein # pip3 install python-levenshtein\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import scholarly #pip3 install scholarly\n",
    "\n",
    "url='https://scholar.google.com'\n",
    "\n",
    "import requests\n",
    "headers_Get = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:49.0) Gecko/20100101 Firefox/49.0',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'DNT': '1',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1'\n",
    "        }\n",
    "r=requests.Session()\n",
    "\n",
    "#import googlescholar as gs\n",
    "#%%writefile ../cienciometria/googlescholar.py\n",
    "\n",
    "def scholarly_to_gs(d):\n",
    "    if not d:\n",
    "        return d\n",
    "    gsr={}\n",
    "    try:\n",
    "        gsr['cites']=d.citedby\n",
    "        gsr['cites_link']=d.id_scholarcitedby\n",
    "    except AttributeError:\n",
    "        gsr['cites']=0\n",
    "        gsr['cites_link']=''\n",
    "    gsr['title']=d.bib.get('title')\n",
    "    gsr['ref']=d.bib.get('author').replace(' and',',')\n",
    "    gsr['authors']=gsr['ref'].split('-')[0].strip()\n",
    "    gsr['PDF']=d.bib.get('eprint')\n",
    "    \n",
    "    try:\n",
    "        jy=gsr['ref'].split('-')[1].strip()\n",
    "        gsr['Journal']=jy.split(',')[0]\n",
    "        try:\n",
    "            gsr['Year']=eval(jy.split(',')[1])\n",
    "        except:\n",
    "            gsr['Year']=-1\n",
    "    except:\n",
    "        gsr['Journal']=''\n",
    "\n",
    "    gsr['abstract']=d.bib.get('abstract')\n",
    "    return gsr\n",
    "\n",
    "def firefox_get(url):\n",
    "\n",
    "    rget=r.get(url,headers=headers_Get)\n",
    "    \n",
    "    html = rget.text\n",
    "    if html.lower().find('gs_captcha_f')>-1:\n",
    "        sys.exit('ERROR: Captcha anti-robots requested!\\n Aborting execution.')\n",
    "\n",
    "    return html\n",
    "\n",
    "def get_google_scholar(record):\n",
    "    '''\n",
    "    Analyise the BeautifulSoup record for an article \n",
    "    in Google Scholar.\n",
    "    Output is a Python dictionary with keys: \n",
    "    'title', 'authors','profiles','Jornal','Year',\n",
    "    'abstract','cites','cites_link'\n",
    "    '''\n",
    "    import random\n",
    "    import time\n",
    "    gsr={}\n",
    "    try:\n",
    "        cites=record.find_all('a',{\"href\":re.compile( \"/scholar\\?cites=\" )})[0]\n",
    "        try:\n",
    "            gsr['cites']=int( cites.text.split()[-1] )\n",
    "            gsr['cites_link']=cites.attrs.get('href')\n",
    "        except:\n",
    "            gsr['cites']=0\n",
    "    except:\n",
    "        cites=None\n",
    "\n",
    "    # Title\n",
    "    try:\n",
    "        #The .split('XXX')[-1]  does not afect the result when .text does no contains 'XXX'\n",
    "        tc=record.find_all('h3',{\"class\":\"gs_rt\"})[0].text.split('[CITATION][C] ')[-1]\n",
    "    except:\n",
    "        tc=''\n",
    "\n",
    "    gsr['title']=tc.strip().split('[HTML][HTML] ')[-1].split(\n",
    "                                  '[PDF][PDF] '  )[-1]\n",
    "    \n",
    "    # Explore authors, google scholar profile - Journal, Year - Publisher\n",
    "    gpa=None\n",
    "    try:\n",
    "        gpa=record.find_all('div',{\"class\":\"gs_a\"})[0]\n",
    "        #Full ref with authors, google scholar profile - Journal, Year - Publisher\n",
    "        ref=gpa.text\n",
    "        gsr['ref']=ref.strip()\n",
    "        refparts=ref.split('\\xa0-')\n",
    "    except IndexError:\n",
    "        gsr['ref']=''\n",
    "        refparts=[]\n",
    "\n",
    "    try:\n",
    "        gsr['authors']=refparts[0]\n",
    "    except IndexError:    \n",
    "        gsr['authors']=''\n",
    "        \n",
    "    try:\n",
    "        journalparts=refparts[-1].strip().split(' - ')\n",
    "        gsr['publisher']=journalparts[-1]\n",
    "        gsr['Journal']=journalparts[0].split(',')[0]\n",
    "        gsr['Year']=journalparts[0].split(',')[-1].strip()\n",
    "    except IndexError:\n",
    "        gsr['publisher']=''\n",
    "        gsr['Journal']=''\n",
    "        gsr['Year']\n",
    "\n",
    "    #Abstract:\n",
    "    try:\n",
    "        gsr['abstract']=record.find_all('div',{'class':'gs_rs'})[0].text.replace('\\xa0…','')\n",
    "    except:\n",
    "        gsr['abstract']=''\n",
    "    # citations\n",
    "    if gpa:\n",
    "        lpr=gpa.find_all(\"a\",{ \"href\":re.compile(\"/citations\\?user=*\")   } )\n",
    "        prf={}\n",
    "        for pr in lpr:\n",
    "            prf[ pr.text ]=pr.attrs.get('href').split('?')[-1].split('&')[0].split('user=')[-1]\n",
    "        gsr['profiles']=prf\n",
    "    \n",
    "    time.sleep( random.randint(1,3)  ) # avoid robots\n",
    "    return gsr\n",
    "\n",
    "def request_google_scholar_url(url):\n",
    "    return requests.get(url)\n",
    "\n",
    "def google_scholar_page(html):\n",
    "    '''\n",
    "    Convert a Google Scholar page into a list\n",
    "    of dictionaries with metadata info\n",
    "    '''\n",
    "    if html.lower().find('gs_captcha_f')>-1:\n",
    "        input('check robots')\n",
    "   \n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    rgs=soup.find_all('div', {'class':'gs_ri' })\n",
    "\n",
    "    citations=[]\n",
    "    for record in rgs:\n",
    "        citations.append( get_google_scholar(record) )\n",
    "        \n",
    "    return citations\n",
    "\n",
    "def google_scholar_query(title='', author='', coauthors=[], DOI='', year=0, publisher='',\n",
    "                         journal='', volume='', issue='', pages=0,\n",
    "                         DEBUG=False,Scholarly=False):\n",
    "    '''\n",
    "    Search Google scholar with `sholar_lookup` full fields.\n",
    "    Only the first result is analized. The output includes \n",
    "    a quality measurements between the query and the results \n",
    "    Output is a Python dictionary with keys: \n",
    "    'title', 'authors','profiles','cites','cites_link',\n",
    "    'quality_title','quality_author'\n",
    "    '''\n",
    "    gs={}\n",
    "    # + → %2B in query formula:\n",
    "    baseurl='https://scholar.google.com/'\n",
    "    q='scholar_lookup?'\n",
    "    \n",
    "    nl=0\n",
    "    if title:\n",
    "        nl=nl+1\n",
    "        q= q+'title={}'.format(title.replace(' ','+'))\n",
    "    if author:\n",
    "        if nl: q= q+'&'\n",
    "        nl=nl+1\n",
    "        q= q+'author={}'.format(author.replace(' ','+'))\n",
    "    if DOI:\n",
    "        if nl: q= q+'&'\n",
    "        nl=nl+1    \n",
    "        q= q+'doi={}'.format(DOI)\n",
    "    if year:\n",
    "        if nl: q= q+'&'\n",
    "        nl=nl+1\n",
    "        q= q+'year={}'.format(year)\n",
    "    if publisher:\n",
    "        if nl: q= q+'&'\n",
    "        nl=nl+1\n",
    "        q= q+'publisher={}'.format(publisher.replace(' ','+'))\n",
    "    if journal:\n",
    "        if nl: q= q+'&'\n",
    "        nl=nl+1     \n",
    "        q= q+'journal={}'.format(journal.replace(' ','+'))\n",
    "    if volume:\n",
    "        if nl: q= q+'&'\n",
    "        nl=nl+1\n",
    "        q= q+'volume={}'.format(volume)\n",
    "    if issue:\n",
    "        if nl: q= q+'&'\n",
    "        nl=nl+1\n",
    "        q= q+'issue={}'.format(issue)   \n",
    "    if pages:\n",
    "        if nl: q= q+'&'\n",
    "        nl=nl+1\n",
    "        q= q+'pages={}'.format(pages)\n",
    "    if coauthors:\n",
    "        for i in coauthors :\n",
    "            if nl: q= q+'&'\n",
    "            nl=nl+1\n",
    "            q= q+'author={}'.format(i.replace(' ','+'))    \n",
    "    url=baseurl+q\n",
    "    if DEBUG:\n",
    "        print(url)   \n",
    "    \n",
    "    #s = requests.Session()\n",
    "    if not Scholarly:\n",
    "        rtext=firefox_get(url)\n",
    "\n",
    "        #soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "        soup = BeautifulSoup(rtext, \"html.parser\")\n",
    "\n",
    "\n",
    "        # Main contents:\n",
    "        rgs=soup.find_all('div', {'class':'gs_ri' })\n",
    "\n",
    "        try:\n",
    "            record=rgs[0]\n",
    "        except IndexError:\n",
    "            #exit if record not found and returns empty dictionary\n",
    "            return gs\n",
    "    \n",
    "        gs.update(get_google_scholar(record))\n",
    "    else:\n",
    "        search_query = scholarly.search_pubs_query(q)\n",
    "\n",
    "        try:\n",
    "            d=next(search_query)\n",
    "        except StopIteration:\n",
    "            return {}     \n",
    "        gs=scholarly_to_gs(d)\n",
    "    \n",
    "    #Check if author is in authors list\n",
    "    if gs and author:\n",
    "        sau=0\n",
    "        for a in gs['authors'].split(','):\n",
    "            saun=Levenshtein.ratio(author.lower(),a.lower().strip())\n",
    "            if saun>sau:\n",
    "                sau=saun\n",
    "                \n",
    "        gs['quality_author']=round(sau,2)\n",
    "    else:\n",
    "        gs['quality_author']=-1 #-1 means not checked\n",
    "        \n",
    "    if gs and title:\n",
    "        gs['quality_title']=round( Levenshtein.ratio(\n",
    "                   title.lower(),gs['title'].lower() ),2 )\n",
    "    else:\n",
    "        gs['quality_title'] =-1 #-1 means not checked\n",
    "        \n",
    "    #EXTRA FIELDS:\n",
    "    #PDF\n",
    "    if not Scholarly:\n",
    "        try:\n",
    "            gs['PDF']=soup.find_all('div',\n",
    "                        {\"class\":\"gs_or_ggsm\"})[0].find_all('a')[0].get(\"href\")\n",
    "        except:\n",
    "            gs['PDF']=''\n",
    "\n",
    "\n",
    "    if DEBUG:\n",
    "        print('='*80)\n",
    "        print(record)\n",
    "        \n",
    "        return gs,record\n",
    "    else:\n",
    "        return gs\n",
    "\n",
    "def get_cites_refs(browser,url,maxcites=65,t=60):\n",
    "    \"\"\"\n",
    "    WARNING: Works only with SELENIUM true\n",
    "    \"\"\"\n",
    "    import random\n",
    "    import time\n",
    "    url='https://scholar.google.com'+url\n",
    "    browser.get(url)\n",
    "     \n",
    "    endpage=int(maxcites/10)+1\n",
    "    refs=''\n",
    "    \n",
    "    kk=google_scholar_page( browser.page_source )\n",
    "    try:\n",
    "        refs=refs+'\\n'.join( list((pd.DataFrame(kk)['title']+'; '\n",
    "                                  +pd.DataFrame(kk)['ref']).values) )+'\\n' \n",
    "    except:\n",
    "        refs=''\n",
    "    \n",
    "    \n",
    "    for i in range(endpage):\n",
    "        try:\n",
    "            browser.find_element_by_class_name('gs_ico_nav_next').click()\n",
    "            kk=google_scholar_page( browser.page_source )\n",
    "            try:\n",
    "                refs=refs+'\\n'.join( list( (pd.DataFrame(kk)['title']+'; '\n",
    "                                +pd.DataFrame(kk)['ref']).values ) )+'\\n' \n",
    "            except:\n",
    "                refs=''\n",
    "        except:\n",
    "            break\n",
    "            \n",
    "    time.sleep(random.uniform(0.9*t,1.1*t))\n",
    "    return refs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wosplus as wp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting drive.cfg\n"
     ]
    }
   ],
   "source": [
    "%%writefile drive.cfg\n",
    "[FILES]\n",
    "DOIS.xlsx               = 1bikNT7Gmp4G7dfeMuGsF-az7D8lskK0O\n",
    "UDEA_WOS_SCI_SCP.xlsx   = 1o9otmklgh-0w18Avv2ZTKOXr3vZbjwvj\n",
    "oaudea.xlsx             = 1CcwobiEFACIbffNzNdLxpdxQukr8cZ5x\n",
    "oaudea.json             = 1BmqRoQDgfpOFjXBVEfI999uaN6XX6Fkd\n",
    "datos1.csv              = 11CyLRZZwVbgw6YAC-igRJ3mrkIwk0aaiXnd-EOofYTI\n",
    "RedalycMetadatosArticulos.csv=# USE Google Drive id here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "oa=wp.wosplus('drive.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oaudea=oa.read_drive_csv('RedalycMetadatosArticulos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "oaudea=wp.fill_NaN(oaudea)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(629851, 13)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oaudea.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos=oaudea.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#JUST START AFTER FINISH REDALYC\n",
    "nini=3000# initial doi\n",
    "n=3000 # Total of DOIs\n",
    "nend=nini+n\n",
    "T=12 #hours of search\n",
    "t=T/n*3600 # [s] query time\n",
    "t=60\n",
    "day=24*3600 #s\n",
    "mintime=0.9*t*n # [s] minimal time search\n",
    "wait=30#day-mintime # maximum wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos=datos[datos.DOI!=''].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['REVISTA', 'INSTITUCION_REVISTA', 'PAIS_REVISTA', 'IDARTICULO',\n",
       "       'TITULO', 'Autor(es)', 'DOI', 'ANIO', 'VOLUMEN', 'NUMERO', 'PAGINAS',\n",
       "       'IDIOMA', 'RESUMEN'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('10.4067/S0717-554X2017000100013',\n",
       " 'Ventajas del análisis sistémico aplicado a los espacios locales',\n",
       " 'Manuela Ortega ,M. Concepción Segovia ',\n",
       " [],\n",
       " 2017,\n",
       " 'Cinta de Moebio')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ii=1\n",
    "(datos.loc[ii,'DOI'],\n",
    " datos.loc[ii,'TITULO'],\n",
    " datos.loc[ii,'Autor(es)'].split(', ')[0],\n",
    " datos.loc[ii,'Autor(es)'].split('\\n')[1:4],\n",
    " datos.loc[ii,'ANIO'],\n",
    " datos.loc[ii,'REVISTA'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18815, 13)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=3000, step=1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos[nini:n].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2017"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos.loc[ii,'ANIO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Journal': 'Cinta de moebio',\n",
       " 'PDF': 'https://scielo.conicyt.cl/scielo.php?pid=S0717-554X2017000100013&script=sci_arttext',\n",
       " 'Year': '2017',\n",
       " 'abstract': 'This paper shows how system analysis applied to local spaces facilitate the generation of proposals for integral development in the field of sustainability. Regions are composed of physical and social components generating products that benefit or harm themselves. These resources can be exploited or remained idle for lack of actors to develop them. They react to internal and external actions, both human and natural. Among these actions are the policies for local development. For these policies to be efficient, they need comprehensive studies',\n",
       " 'authors': 'M Ortega, MC Segovia',\n",
       " 'cites': 1,\n",
       " 'cites_link': '/scholar?cites=14403974261697570786&as_sdt=2005&sciodt=0,5&hl=en',\n",
       " 'profiles': {'M Ortega': 'GkYFaLMAAAAJ'},\n",
       " 'publisher': 'scielo.conicyt.cl',\n",
       " 'quality_author': 0.42,\n",
       " 'quality_title': 1.0,\n",
       " 'ref': 'M Ortega, MC Segovia\\xa0- Cinta de moebio, 2017 - scielo.conicyt.cl',\n",
       " 'title': 'Ventajas del análisis sistémico aplicado a los espacios locales'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "Scholarly=False\n",
    "doi='DOI'\n",
    "title_simple='TITULO'\n",
    "article_id='IDARTICULO'\n",
    "\n",
    "d=google_scholar_query(title=datos.loc[ii,title_simple],\n",
    "                    author=datos.loc[ii,'Autor(es)'].split(', ')[0],\n",
    "                    coauthors=datos.loc[ii,'Autor(es)'].split(', ')[1:4],\n",
    "                    DOI=datos.loc[ii,doi],\n",
    "                    year=datos.loc[ii,'ANIO'],\n",
    "                    journal=datos.loc[ii,'REVISTA'],Scholarly=Scholarly)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "Scholarly=False\n",
    "doi='DOI'\n",
    "title_simple='TITULO'\n",
    "article_id='IDARTICULO'\n",
    "dfgs=pd.DataFrame()\n",
    "ibrkn=0\n",
    "maxibrn=400\n",
    "\n",
    "for ii in datos[nini:nend].index:\n",
    "    print(ii,datos.loc[ii,doi])\n",
    "    #gsd=google_scholar_query(DOI=doi)\n",
    "    gsd=google_scholar_query(title=datos.loc[ii,title_simple],\n",
    "                    author=datos.loc[ii,'Autor(es)'].split(', ')[0],\n",
    "                    coauthors=datos.loc[ii,'Autor(es)'].split(', ')[1:4],\n",
    "                    DOI=datos.loc[ii,doi],\n",
    "                    year=datos.loc[ii,'ANIO'],\n",
    "                    journal=datos.loc[ii,'REVISTA'],Scholarly=Scholarly )\n",
    "            \n",
    "    gsd['old_title']=datos.loc[ii,'TITULO']\n",
    "    gsd['DOI']=datos.loc[ii,'DOI']\n",
    "    gsd['ID_ARTICLE']=datos.loc[ii,article_id]\n",
    "    dfgs=dfgs.append(gsd,ignore_index=True )\n",
    "    dfgs.to_json('rdlycdoi_{}_{}.json'.format(nini,nend))\n",
    "    time.sleep(random.uniform(0.9*t,1.1*t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of OpenAccess.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
