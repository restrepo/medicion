{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uu-sf4Ukvbb-"
   },
   "source": [
    "# Google Scholar \n",
    "By using lookup fields:\n",
    "\n",
    "<a href=\"http://bit.ly/gslookup\">https://scholar.google.co.in/scholar_lookup?<font color=\"red\">title=</font>Estimates+for+the+number+of+sums+and+products+and+for+exponential+sums+in+fields+of+prime+order&<font color=\"red\">author=</font>Jean+Bourgain&author=AA+Glibichuk&<font color=\"red\">author=</font>SERGEI+VLADIMIROVICH+Konyagin&<font color=\"red\">year=</font>2006&<font color=\"red\">doi=</font>10.1112/S0024610706022721&<font color=\"red\">publisher=</font>Oxford+University+Press&<font color=\"red\">journal=</font>Journal+of+the+London+Mathematical+Society&<font color=\"red\">volume=</font>73&<font color=\"red\">issue=</font>2&<font color=\"red\">pages=</font>380-398&hl=en</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: convert to script: https://stackoverflow.com/a/50468400/2268280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein # pip3 install python-levenshtein\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url='https://scholar.google.com'\n",
    "\n",
    "import requests\n",
    "headers_Get = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:49.0) Gecko/20100101 Firefox/49.0',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'DNT': '1',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1'\n",
    "        }\n",
    "\n",
    "\n",
    "#import googlescholar as gs\n",
    "#%%writefile ../cienciometria/googlescholar.py\n",
    "\n",
    "def firefox_get(url):\n",
    "    r=requests.Session()\n",
    "    rget=r.get(url,headers=headers_Get)\n",
    "    \n",
    "    html = rget.text\n",
    "    if html.lower().find('gs_captcha_f')>-1:\n",
    "        sys.exit('ERROR: Captcha anti-robots requested!\\n Aborting execution.')\n",
    "\n",
    "    return html\n",
    "\n",
    "def get_google_scholar(record):\n",
    "    '''\n",
    "    Analyise the BeautifulSoup record for an article \n",
    "    in Google Scholar.\n",
    "    Output is a Python dictionary with keys: \n",
    "    'title', 'authors','profiles','Jornal','Year',\n",
    "    'abstract','cites','cites_link'\n",
    "    '''\n",
    "    import random\n",
    "    import time\n",
    "    gsr={}\n",
    "    try:\n",
    "        cites=record.find_all('a',{\"href\":re.compile( \"/scholar\\?cites=\" )})[0]\n",
    "        try:\n",
    "            gsr['cites']=int( cites.text.split()[-1] )\n",
    "            gsr['cites_link']=cites.attrs.get('href')\n",
    "        except:\n",
    "            gsr['cites']=0\n",
    "    except:\n",
    "        cites=None\n",
    "\n",
    "    # Title\n",
    "    try:\n",
    "        #The .split('XXX')[-1]  does not afect the result when .text does no contains 'XXX'\n",
    "        tc=record.find_all('h3',{\"class\":\"gs_rt\"})[0].text.split('[CITATION][C] ')[-1]\n",
    "    except:\n",
    "        tc=''\n",
    "\n",
    "    gsr['title']=tc.strip().split('[HTML][HTML] ')[-1].split(\n",
    "                                  '[PDF][PDF] '  )[-1]\n",
    "    \n",
    "    # Explore authors, google scholar profile - Journal, Year - Publisher\n",
    "    gpa=None\n",
    "    try:\n",
    "        gpa=record.find_all('div',{\"class\":\"gs_a\"})[0]\n",
    "        #Full ref with authors, google scholar profile - Journal, Year - Publisher\n",
    "        ref=gpa.text\n",
    "        gsr['ref']=ref.strip()\n",
    "        refparts=ref.split('\\xa0-')\n",
    "    except IndexError:\n",
    "        gsr['ref']=''\n",
    "        refparts=[]\n",
    "\n",
    "    try:\n",
    "        gsr['authors']=refparts[0]\n",
    "    except IndexError:    \n",
    "        gsr['authors']=''\n",
    "        \n",
    "    try:\n",
    "        journalparts=refparts[-1].strip().split(' - ')\n",
    "        gsr['publisher']=journalparts[-1]\n",
    "        gsr['Journal']=journalparts[0].split(',')[0]\n",
    "        gsr['Year']=journalparts[0].split(',')[-1].strip()\n",
    "    except IndexError:\n",
    "        gsr['publisher']=''\n",
    "        gsr['Journal']=''\n",
    "        gsr['Year']\n",
    "\n",
    "    #Abstract:\n",
    "    try:\n",
    "        gsr['abstract']=record.find_all('div',{'class':'gs_rs'})[0].text.replace('\\xa0…','')\n",
    "    except:\n",
    "        gsr['abstract']=''\n",
    "    # profiles\n",
    "    if gpa:\n",
    "        lpr=gpa.find_all(\"a\",{ \"href\":re.compile(\"/citations\\?user=*\")   } )\n",
    "        prf={}\n",
    "        for pr in lpr:\n",
    "            prf[ pr.text ]=pr.attrs.get('href').split('?')[-1].split('&')[0].split('user=')[-1]\n",
    "        gsr['profiles']=prf\n",
    "    \n",
    "    time.sleep( random.randint(1,3)  ) # avoid robots\n",
    "    return gsr\n",
    "\n",
    "def request_google_scholar_url(url):\n",
    "    return requests.get(url)\n",
    "\n",
    "def google_scholar_page(html):\n",
    "    '''\n",
    "    Convert a Google Scholar page into a list\n",
    "    of dictionaries with metadata info\n",
    "    '''\n",
    "    if html.lower().find('gs_captcha_f')>-1:\n",
    "        input('check robots')\n",
    "   \n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    rgs=soup.find_all('div', {'class':'gs_ri' })\n",
    "\n",
    "    citations=[]\n",
    "    for record in rgs:\n",
    "        citations.append( get_google_scholar(record) )\n",
    "        \n",
    "    return citations\n",
    "\n",
    "def google_scholar_query(title='', author='', coauthors=[], DOI='', year=0, publisher='',\n",
    "                         journal='', volume='', issue='', pages=0,\n",
    "                         DEBUG=False):\n",
    "    '''\n",
    "    Search Google scholar with `sholar_lookup` full fields.\n",
    "    Only the first result is analized. The output includes \n",
    "    a quality measurements between the query and the results \n",
    "    Output is a Python dictionary with keys: \n",
    "    'title', 'authors','profiles','cites','cites_link',\n",
    "    'quality_title','quality_author'\n",
    "    '''\n",
    "    gs={}\n",
    "    # + → %2B in query formula:\n",
    "    baseurl='https://scholar.google.com/'\n",
    "    q='scholar_lookup?'\n",
    "    \n",
    "    nl=0\n",
    "    if title:\n",
    "        nl=nl+1\n",
    "        q= q+'title={}'.format(title.replace(' ','+'))\n",
    "    if author:\n",
    "        if nl: q= q+'&'\n",
    "        nl=nl+1\n",
    "        q= q+'author={}'.format(author.replace(' ','+'))\n",
    "    if DOI:\n",
    "        if nl: q= q+'&'\n",
    "        nl=nl+1    \n",
    "        q= q+'doi={}'.format(DOI)\n",
    "    if year:\n",
    "        if nl: q= q+'&'\n",
    "        nl=nl+1\n",
    "        q= q+'year={}'.format(year)\n",
    "    if publisher:\n",
    "        if nl: q= q+'&'\n",
    "        nl=nl+1\n",
    "        q= q+'publisher={}'.format(publisher.replace(' ','+'))\n",
    "    if journal:\n",
    "        if nl: q= q+'&'\n",
    "        nl=nl+1     \n",
    "        q= q+'journal={}'.format(journal.replace(' ','+'))\n",
    "    if volume:\n",
    "        if nl: q= q+'&'\n",
    "        nl=nl+1\n",
    "        q= q+'volume={}'.format(volume)\n",
    "    if issue:\n",
    "        if nl: q= q+'&'\n",
    "        nl=nl+1\n",
    "        q= q+'issue={}'.format(issue)   \n",
    "    if pages:\n",
    "        if nl: q= q+'&'\n",
    "        nl=nl+1\n",
    "        q= q+'pages={}'.format(pages)\n",
    "    if coauthors:\n",
    "        for i in coauthors :\n",
    "            if nl: q= q+'&'\n",
    "            nl=nl+1\n",
    "            q= q+'author={}'.format(i.replace(' ','+'))    \n",
    "    url=baseurl+q\n",
    "    if DEBUG:\n",
    "        print(url)   \n",
    "    \n",
    "    #s = requests.Session()\n",
    "    rtext=firefox_get(url)\n",
    "\n",
    "    #soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    soup = BeautifulSoup(rtext, \"html.parser\")\n",
    "\n",
    "\n",
    "    # Main contents:\n",
    "    rgs=soup.find_all('div', {'class':'gs_ri' })\n",
    "\n",
    "    try:\n",
    "        record=rgs[0]\n",
    "    except IndexError:\n",
    "        #exit if record not found and returns empty dictionary\n",
    "        return gs\n",
    "    \n",
    "    gs.update(get_google_scholar(record))\n",
    "    \n",
    "    #Check if author is in authors list\n",
    "    if gs and author:\n",
    "        sau=0\n",
    "        for a in gs['authors'].split(','):\n",
    "            saun=Levenshtein.ratio(author.lower(),a.lower().strip())\n",
    "            if saun>sau:\n",
    "                sau=saun\n",
    "                \n",
    "        gs['quality_author']=round(sau,2)\n",
    "    else:\n",
    "        gs['quality_author']=-1 #-1 means not checked\n",
    "        \n",
    "    if gs and title:\n",
    "        gs['quality_title']=round( Levenshtein.ratio(\n",
    "                   title.lower(),gs['title'].lower() ),2 )\n",
    "    else:\n",
    "        gs['quality_title'] =-1 #-1 means not checked\n",
    "        \n",
    "    #EXTRA FIELDS:\n",
    "    #PDF\n",
    "    try:\n",
    "        gs['PDF']=soup.find_all('div',\n",
    "                        {\"class\":\"gs_or_ggsm\"})[0].find_all('a')[0].get(\"href\")\n",
    "    except:\n",
    "        gs['PDF']=''\n",
    "\n",
    "\n",
    "    if DEBUG:\n",
    "        print('='*80)\n",
    "        print(record)\n",
    "        \n",
    "        return gs,record\n",
    "    else:\n",
    "        return gs\n",
    "\n",
    "def get_cites_refs(browser,url,maxcites=65,t=60):\n",
    "    \"\"\"\n",
    "    WARNING: Works only with SELENIUM true\n",
    "    \"\"\"\n",
    "    import random\n",
    "    import time\n",
    "    url='https://scholar.google.com'+url\n",
    "    browser.get(url)\n",
    "     \n",
    "    endpage=int(maxcites/10)+1\n",
    "    refs=''\n",
    "    \n",
    "    kk=google_scholar_page( browser.page_source )\n",
    "    try:\n",
    "        refs=refs+'\\n'.join( list((pd.DataFrame(kk)['title']+'; '\n",
    "                                  +pd.DataFrame(kk)['ref']).values) )+'\\n' \n",
    "    except:\n",
    "        refs=''\n",
    "    \n",
    "    \n",
    "    for i in range(endpage):\n",
    "        try:\n",
    "            browser.find_element_by_class_name('gs_ico_nav_next').click()\n",
    "            kk=google_scholar_page( browser.page_source )\n",
    "            try:\n",
    "                refs=refs+'\\n'.join( list( (pd.DataFrame(kk)['title']+'; '\n",
    "                                +pd.DataFrame(kk)['ref']).values ) )+'\\n' \n",
    "            except:\n",
    "                refs=''\n",
    "        except:\n",
    "            break\n",
    "            \n",
    "    time.sleep(random.uniform(0.9*t,1.1*t))\n",
    "    return refs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "apikey=getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#JUST START AFTER FINISH REDALYC\n",
    "nini=0# initial doi\n",
    "n=3000 # Total of DOIs\n",
    "nend=nini+n\n",
    "T=12 #hours of search\n",
    "t=T/n*3600 # [s] query time\n",
    "t=60\n",
    "day=24*3600 #s\n",
    "mintime=0.9*t*n # [s] minimal time search\n",
    "wait=30#day-mintime # maximum wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "#See https://stackoverflow.com/questions/36837663/reading-json-file-as-pandas-dataframe-error\n",
    "r = requests.get('http://fisica.udea.edu.co:8080/data?init={}&end={}&apikey={}'.format(\n",
    "           nini,nend+1,apikey)) # Fix bug in API\n",
    "df=pd.DataFrame(json.loads(r.text)).fillna('').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ANIO', 'Autor(es)', 'DOI', 'IDARTICULO', 'IDIOMA',\n",
       "       'INSTITUCION_REVISTA', 'NUMERO', 'PAGINAS', 'PAIS_REVISTA', 'RESUMEN',\n",
       "       'REVISTA', 'TITULO', 'VOLUMEN', '_id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datos.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('',\n",
       " 'Política curricular, crisis de legitimación y hegemonía neoliberal. Una visión desde la sociología de la educación crítica',\n",
       " 'Guillermo  Miranda Camacho',\n",
       " [],\n",
       " 2007,\n",
       " 'Revista de Ciencias Sociales (Cr)')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ii=0\n",
    "(datos.loc[ii,'DOI'],\n",
    " datos.loc[ii,'TITULO'].replace('\\n',' ').strip(),\n",
    " datos.loc[ii,'Autor(es)'].split(', ')[0].strip(),\n",
    " [x.strip() for x in datos.loc[ii,'Autor(es)'].split('\\n')[1:4]],\n",
    " datos.loc[ii,'ANIO'],\n",
    " datos.loc[ii,'REVISTA'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ii=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Journal': 'Revista de Ciencias Sociales',\n",
       " 'PDF': 'https://revistas.ucr.ac.cr/index.php/sociales/article/download/11206/10564',\n",
       " 'Year': '2007',\n",
       " 'abstract': 'Este artículo realiza una aproximación hermenéutica, en el marco de la sociología de la educación crítica, al papel de la política curricular y del conocimiento oficial en el proceso de configuración del sistema hegemónico neoliberal, que tuvo lugar como resultado de la crisis de legitimación del capitalismo regulado y del Estado del bienestar. Contiene los siguientes núcleos temáticos: 1) el papel del currículo en las crisis de legitimación desde la teoría de la crisis de legitimación de Habermas y Offe; 2) la alianza de la nueva derecha',\n",
       " 'authors': 'GM Camacho',\n",
       " 'cites': 10,\n",
       " 'cites_link': '/scholar?cites=16407895324746222600&as_sdt=2005&sciodt=0,5&hl=en',\n",
       " 'profiles': {},\n",
       " 'publisher': 'revistas.ucr.ac.cr',\n",
       " 'quality_author': 0.56,\n",
       " 'quality_title': 0.99,\n",
       " 'ref': 'GM Camacho\\xa0- Revista de Ciencias Sociales, 2007 - revistas.ucr.ac.cr',\n",
       " 'title': 'Política curricular, crisis de legitimación y hegemonía neoliberal: una visión desde la sociología de la educación crítica'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "doi='DOI'\n",
    "title_simple='TITULO'\n",
    "article_id='IDARTICULO'\n",
    "\n",
    "d=google_scholar_query(title=datos.loc[ii,title_simple].replace('\\n',' '),\n",
    "                    author=datos.loc[ii,'Autor(es)'].split(', ')[0].strip(),\n",
    "                    coauthors=   datos.loc[ii,'Autor(es)'].split(', ')[1:4],\n",
    "                    DOI=datos.loc[ii,doi],\n",
    "                    year=datos.loc[ii,'ANIO'],\n",
    "                    journal=datos.loc[ii,'REVISTA'],\n",
    "                    volume=datos.loc[ii,'VOLUMEN'],\n",
    "                    issue=datos.loc[ii,'NUMERO'],\n",
    "                    pages=datos.loc[ii,'PAGINAS']\n",
    "                      )\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "doi='DOI'\n",
    "title_simple='TITULO'\n",
    "article_id='IDARTICULO'\n",
    "dfgs=pd.DataFrame()\n",
    "ibrkn=0\n",
    "maxibrn=400\n",
    "\n",
    "for ii in datos[nini:nend].index:\n",
    "    print(ii,datos.loc[ii,doi])\n",
    "    #gsd=google_scholar_query(DOI=doi)\n",
    "    gsd=google_scholar_query(title=datos.loc[ii,title_simple].replace('\\n',' '),\n",
    "                    author=datos.loc[ii,'Autor(es)'].split(', ')[0].strip(),\n",
    "                    coauthors=datos.loc[ii,'Autor(es)'].split(', ')[1:4],\n",
    "                    DOI=datos.loc[ii,doi],\n",
    "                    year=datos.loc[ii,'ANIO'],\n",
    "                    journal=datos.loc[ii,'REVISTA'],\n",
    "                    volume=datos.loc[ii,'VOLUMEN'],\n",
    "                    issue=datos.loc[ii,'NUMERO'],\n",
    "                    pages=datos.loc[ii,'PAGINAS'])\n",
    "            \n",
    "    gsd['old_title']=datos.loc[ii,'TITULO']\n",
    "    gsd['DOI']=datos.loc[ii,'DOI']\n",
    "    gsd['ID_ARTICLE']=datos.loc[ii,article_id]\n",
    "    dfgs=dfgs.append(gsd,ignore_index=True )\n",
    "    dfgs.to_json('rdlyc_{}_{}.json'.format(nini,nend))\n",
    "    time.sleep(random.uniform(0.9*t,1.1*t))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of OpenAccess.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "512px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
