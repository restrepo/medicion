{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "merge.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/restrepo/medicion/blob/master/cienciometria/tmp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "aDnqho-CsEwj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# WOS+SCI+SCP\n",
        "Merge the bibliographic datasets for Web of Scienca, Scielo and Scopus of the scientific articles of Universidad de Antioquia\n",
        "\n",
        "For details see [merge.ipynb in Colaboratory](https://colab.research.google.com/github/restrepo/medicion/blob/master/cienciometria/merge.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "lOGedSKR89du",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "ed240981-31a2-4138-9f4a-4b5889eff20e"
      },
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JJUmy1YqAszI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i2T9HjxIAulU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "13a4ee1e-729d-495a-9b49-ad80e936af6e"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/\")\n",
        "if os.path.isdir('wosplus'):\n",
        "    shutil.rmtree('wosplus')\n",
        "! git clone https://github.com/restrepo/wosplus.git\n",
        "os.chdir('wosplus/wosplus')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'wosplus'...\n",
            "remote: Enumerating objects: 380, done.\u001b[K\n",
            "remote: Total 380 (delta 0), reused 0 (delta 0), pack-reused 380\u001b[K\n",
            "Receiving objects: 100% (380/380), 176.34 KiB | 992.00 KiB/s, done.\n",
            "Resolving deltas: 100% (236/236), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nhk2ZGEDd2Yo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import wosplus as wp\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JS7jD1f47JUN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##  Configure public links of  files in Google Drive\n",
        "* If it is a Google Spreadsheet the corresponding file is downloaded as CSV\n",
        "* If it is in excel or text file the file is downloaded  directly\n",
        "\n",
        "To define your  own labeled IDs for public google drive files edit the next cell:"
      ]
    },
    {
      "metadata": {
        "id": "mfYH9hg84p2T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8195
        },
        "outputId": "e152d7d4-5296-41c0-9ab0-c6258f08b665"
      },
      "cell_type": "code",
      "source": [
        "cat wosplus.py"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#!/usr/bin/env python3\n",
            "import re\n",
            "import pandas as pd\n",
            "from configparser import ConfigParser\n",
            "try:\n",
            "    from ._google_drive_tools import *\n",
            "    from ._pajek_tools import *\n",
            "    from ._wos_scp import *\n",
            "    from ._merge_tools import *\n",
            "    from ._wos_parser import *\n",
            "except (SystemError, ImportError):\n",
            "    from _google_drive_tools import *\n",
            "    from _pajek_tools import *\n",
            "    from _wos_scp import *\n",
            "    from _merge_tools import *\n",
            "    from _wos_parser import *\n",
            "\n",
            "#TODO: change Tipo for Type or something similar\n",
            "#pd.set_option('display.max_rows', 500)\n",
            "#pd.set_option('display.max_columns', 500)\n",
            "#pd.set_option('display.max_colwidth',1000)\n",
            "def grep(pattern,multilinestring):\n",
            "    '''Grep replacement in python\n",
            "    as in: $ echo $multilinestring | grep pattern\n",
            "    dev: re.M is for multiline strings\n",
            "    '''\n",
            "    grp=re.finditer('(.*)%s(.*)' %pattern, multilinestring,re.M)\n",
            "    return '\\n'.join([g.group(0) for g in grp])\n",
            "\n",
            "def merge_inner_interior_exterior(LEFT,RIGHT,on_condition='SCP_DOI',left_on='ST',right_on='Simple_Title',\\\n",
            "            left_series=pd.Series(),right_series=pd.Series(),\\\n",
            "            left_extra_on='SO',right_extra_on='UDEA_nombre revista o premio',\\\n",
            "            close_matches=False,cutoff=0.6,cutoff_extra=0.6):\n",
            "    '''\n",
            "    Given a df LEFT and a RIGHT[RIGHT[on_condition]!=''] fully True, then\n",
            "    Get a tuple with the following 3 dataframes\n",
            "    1) left-right intersection (inner): LR\n",
            "    1) pure left (interior): L-LR\n",
            "    3) pure right: R-LR (exterior)\n",
            "    '''\n",
            "    if LEFT.shape[0]==0:\n",
            "        print('All entries matched',r,l)\n",
            "        return pd.DataFrame(),pd.DataFrame(),RIGHT\n",
            "    \n",
            "    RIGHT=RIGHT[RIGHT[on_condition]!='']\n",
            "    if RIGHT.shape[0]:\n",
            "        interior,inner,exterior=merge_by_series(LEFT.copy(),RIGHT.copy(),left_on=left_on,right_on=right_on,\\\n",
            "                                   left_series=left_series,right_series=right_series,\\\n",
            "                                   left_extra_on=left_extra_on,right_extra_on=right_extra_on,\\\n",
            "                                   close_matches=close_matches,cutoff=cutoff,cutoff_extra=cutoff_extra)\n",
            "        if LEFT.shape[0]>=interior.shape[0] and RIGHT.shape[0]>=exterior.shape[0]:\n",
            "            return inner.reset_index(drop=True),interior.reset_index(drop=True),exterior.reset_index(drop=True)\n",
            "    else:\n",
            "        return (pd.DataFrame(),pd.DataFrame(),pd.DataFrame())\n",
            "\n",
            "\n",
            "#Start here\n",
            "class wosplus:\n",
            "    \"\"\"\n",
            "    Input files assumed to have public links in Google Drive\n",
            "    A config file, e.g 'drive.cfg' is expected with the following structure\n",
            "    ==============================================================\n",
            "    [FILES]\n",
            "    WOS_FILE.xlsx              = 1--LJZ4mYyQcaJ93xBdbnYj-ZzdjO2Wq2\n",
            "    ...\n",
            "    ==============================================================\n",
            "    USAGE:\n",
            "        import wosplus as wp\n",
            "        WOS=wp.wosplus('drive.cfg')\n",
            "        #check Google Drive id for file\n",
            "        WOS.drive_file.get('WOS_FILE.xlsx')\n",
            "        #load biblio\n",
            "        WOS.load_biblio('WOS_FILE.xlsx')\n",
            "        # DataFrame in WOS.WOS or WOS.biblio['WOS']\n",
            "  \n",
            "    The main method is 'load_biblio' it must have a prefix according to\n",
            "    the type of supported bibliography. This prefix will be appended\n",
            "    to the ALL the columns of the generated bibligraphy\n",
            "\n",
            "    wp.type gives the type of bibliography Data Base.\n",
            "    Currently implenented:\n",
            "      * WOS: two characters columns\n",
            "      * SCI: prefixed SCI_ columns is returned\n",
            "      * SCP: prefixed SCP_ columns is returned \n",
            "    and any combinantion\n",
            "    of them keeping the same ordering.\n",
            "\n",
            "    The type mus be declared with the 'load_biblio' with the 'prefix' option\n",
            "    (Default type is WOS)\n",
            "    \"\"\"\n",
            "\n",
            "    def __init__(self,cfg_file=''):\n",
            "        self.df=pd.DataFrame()\n",
            "        '''\n",
            "        Based on:\n",
            "        http://stackoverflow.com/a/39225272\n",
            "        '''\n",
            "        cfg=ConfigParser()\n",
            "        cfg.optionxform=str\n",
            "        if cfg_file:\n",
            "            tmp=cfg.read(cfg_file)\n",
            "        else:\n",
            "            tmp=cfg.read_dict({'FILES':\n",
            "                    {'Sample_WOS.xlsx':'0BxoOXsn2EUNIMldPUFlwNkdLOTQ'}})\n",
            "            \n",
            "        self.drive_file=cfg['FILES']\n",
            "        self.type=pd.Series()\n",
            "        self.biblio=pd.Series()\n",
            "    def read_drive_file(self,file_name):\n",
            "        '''\n",
            "        Convert Google Drive File in a Pyton IO file object\n",
            "        \n",
            "         Requires a self.drive_file dictionary intialized with the class.\n",
            "         See read_drive_excel help\n",
            "        '''\n",
            "        return download_file_from_google_drive(\n",
            "                self.drive_file.get(file_name) )\n",
            "    def read_drive_excel(self,file_name,**kwargs):\n",
            "        '''\n",
            "        TODO: Make independent of the class!\n",
            "        Generalization of the Pandas DataFrame read_excel method\n",
            "        to include google drive file names:\n",
            "         \n",
            "         Read excel or csv file from google drive\n",
            "         Requires a self.drive_file dictionary intialized with the class\n",
            "         (see below) with the id's for the\n",
            "         google drive file names.\n",
            "         If the file_name is not found in the drive_file dictionary it is read locally.\n",
            "         If the file_name have an extension .csv, try to read the google spreadsheet\n",
            "         directly: check pandas_from_google_drive_csv for passed options\n",
            "         WARNING: ONLY OLD Google Spread Sheet allows to load sheet different from 0\n",
            "\n",
            "         drive_file dictionary: for some file, e.g 'drive.cfg' the format must be:\n",
            "          $ cat drive.cfg\n",
            "          [FILES]\n",
            "          Sample_WOS.xlsx = 0BxoOXsn2EUNIMldPUFlwNkdLOTQ\n",
            "        '''\n",
            "        # Try to load Google spreadsheet if extension is csv\n",
            "        if re.search('\\.csv$',file_name):\n",
            "            if self.drive_file.get(file_name):\n",
            "                return pandas_from_google_drive_csv(\n",
            "                    self.drive_file.get(file_name),**kwargs)\n",
            "            else:\n",
            "                return pd.read_csv(file_name,**kwargs)\n",
            "       \n",
            "        # Try to load xlsx file if file extension is not csv\n",
            "        if self.drive_file.get(file_name):\n",
            "            return pd.read_excel( self.read_drive_file(file_name) ,**kwargs)  # ,{} is an accepted option\n",
            "        else:\n",
            "            return pd.read_excel(file_name,**kwargs)\n",
            "        \n",
            "    def read_drive_json(self,file_name,**kwargs):\n",
            "        '''\n",
            "        Generalization of the Pandas DataFrame read_json method\n",
            "        to include google drive file names:\n",
            "         \n",
            "         Requires a self.drive_file dictionary intialized with the class.\n",
            "         See read_drive_excel help\n",
            "        '''\n",
            "        if self.drive_file.get(file_name):\n",
            "            return pd.read_json( self.read_drive_file(file_name),**kwargs)\n",
            "        else:\n",
            "            return pd.read_json(file_name,**kwargs)\n",
            "\n",
            "    def read_drive_csv(self,file_name,**kwargs):\n",
            "        '''\n",
            "        Generalization of the Pandas DataFrame read_csv method\n",
            "        to include google drive file names:\n",
            "         \n",
            "         Requires a self.drive_file dictionary intialized with the class.\n",
            "         See read_drive_excel help\n",
            "        '''\n",
            "        if self.drive_file.get(file_name):\n",
            "            return pd.read_csv( self.read_drive_file(file_name),**kwargs)\n",
            "        else:\n",
            "            return pd.read_csv(file_name,**kwargs)\n",
            "        \n",
            "    def load_biblio(self,WOS_file,prefix='WOS'):\n",
            "        \"\"\"\n",
            "        Load WOS xlsx file, or if prefix is given:\n",
            "          prefix='SCI': Load SCI xlsx file and append the 'SCI_' prefix in each column\n",
            "          prefix='SCP': Load SCI csv file and append the 'SCP_' prefix in each column\n",
            "        and add the WOS, SCI, or SCP  attribute to self.\n",
            "        \"\"\"\n",
            "        from pathlib import Path\n",
            "        import sys\n",
            "        DOI='DI'\n",
            "        if prefix=='SCP': #Only if pure scopus\n",
            "            DOI='DOI'\n",
            "        #elif: #Other no WOS-like pures\n",
            "\n",
            "        if not re.search('\\.txt$',WOS_file):\n",
            "            if re.search('\\.json$',WOS_file):\n",
            "                WOS=self.read_drive_json(WOS_file)\n",
            "            else:\n",
            "                WOS=self.read_drive_excel(WOS_file)\n",
            "        else:\n",
            "            id_google_drive=self.drive_file.get('{}'.format(WOS_file))\n",
            "            if id_google_drive:                                     \n",
            "                wos_txt=download_file_from_google_drive(  id_google_drive )#,binary=False)\n",
            "                WOS=wos_to_list_to_pandas(wos_txt)\n",
            "            else: #check local file\n",
            "                my_file = Path(WOS_file)\n",
            "                if my_file.is_file():\n",
            "                    WOS=wos_parser(WOS_file)\n",
            "                else:\n",
            "                    sys.exit('WOS File: {}, NOT FOUND!'.format(WOS_file))\n",
            "\n",
            "        WOS=fill_NaN(WOS)\n",
            "        if prefix=='SCI':\n",
            "            exec('self.{}_not_prefix=WOS'.format(prefix))\n",
            "        \n",
            "        \n",
            "        if 'DI' in WOS and 'TI' in WOS and 'SO' in WOS:\n",
            "            WOS['DI']=WOS['DI'].str.strip()\n",
            "            WOS['TI']=WOS['TI'].str.strip().str.replace('\\n',' ')\n",
            "            WOS['SO']=WOS['SO'].str.strip()\n",
            "        if 'X1' in WOS:\n",
            "            WOS_ti,WOS_not_ti=df_split(WOS,on='TI',on_not_condition=True)\n",
            "            WOS_not_ti['TI']=WOS_not_ti['X1']\n",
            "            WOS=WOS_ti.append(WOS_not_ti).reset_index(drop=True)\n",
            "        \n",
            "        if 'TI' in WOS:\n",
            "            WOS=WOS.drop_duplicates('TI')\n",
            "\n",
            "        #Drop duplicated DOIS\n",
            "        if DOI in WOS:\n",
            "            WOS_di,WOS_not_di=df_split(WOS,on=DOI,on_not_condition=True)\n",
            "            WOS=WOS_not_di.append(WOS_di.drop_duplicates(DOI)).reset_index(drop=True)\n",
            "\n",
            "            \n",
            "        if prefix != 'WOS' and not re.search('_',prefix):\n",
            "            #check if already present\n",
            "            WOS=columns_add_prefix(WOS,prefix)\n",
            "\n",
            "        # Without prefix columns\n",
            "        if 'Tipo' not in WOS and not re.search('_',prefix):\n",
            "            WOS['Tipo']=prefix\n",
            "        else:\n",
            "            print('WARNING: Biblio already has a \"Tipo\" column')\n",
            "            \n",
            "        exec('self.{}=WOS'.format(prefix))\n",
            "        self.type['{}'.format(prefix)]='{}'.format(prefix)\n",
            "        self.biblio['{}'.format(prefix)]=WOS\n",
            "        \n",
            "    def merge(self,left='WOS',right='SCI',\n",
            "                   right_DOI=None,right_TI=None,right_extra_journal=None,\n",
            "                   right_author=None,right_year=None,\n",
            "              DEBUG=False):\n",
            "        \"\"\"\n",
            "        Merge left and right bibliographic dataframes by TYPE and with \n",
            "        Python merge ooption: `how='outer'`.\n",
            "        \n",
            "        The TYPE must coincide with the Object attribute Dataframe: eg:\n",
            "        `left='WOS'` imply that WOS must be an attribute of\n",
            "        self: self.WOS\n",
            "        `right='SCI'` imply that WOS must be an attribute of\n",
            "        self: self.WOS\n",
            "        The DataFrame attributtes of the object `self` are populated by using\n",
            "          `self.loadbiblio(file)`: See `self` help for further instructions.\n",
            "\n",
            "        The self.right DataFrame need to have some mandatories columns:\n",
            "         [[right_DOI,right_TI,right_extra_journal,right_author,right_year]]\n",
            "         They are automatically defined for self.righ TYPE: SCI and SCP and\n",
            "         must be given for other TYPE\n",
            "        \n",
            "        Output:\n",
            "        The resulting DataFrame is returned as:\n",
            "          * self.left_right # with strings names converted into variable names\n",
            "          * self.bibilio['left_right'] # pd.Series\n",
            "        and also the new resulting TYPE is stored as\n",
            "          * self.Tipo['left_right'] -> 'left_right' # pd.Series\n",
            "          \n",
            "        The merged DOI, Titles and Journal Names are stored in\n",
            "        the WOS like `self.left_right` columns: DI,TI,SO with `self.left`\n",
            "        priority for the values.\n",
            "        \"\"\"\n",
            "        if not hasattr(self,left) or not hasattr(self,right):\n",
            "            sys.exit('ERROR:  {} and {} must be attributes of class {}'.format(left,right,self.__class__.__name__))\n",
            "            \n",
            "        if left not in self.biblio or right not in self.biblio:\n",
            "            sys.exit('ERROR: missing biblio Series in {}'.format(left,right,self.__class__.__name__)  )\n",
            "\n",
            "        if left not in self.type or right not in self.type:\n",
            "            sys.exit('ERROR: missing type Series in {}'.format(left,right,self.__class__.__name__)  )\n",
            "            \n",
            "        left_df=self.biblio[left].copy()\n",
            "        right_df=self.biblio[right].copy()\n",
            "        if DEBUG:\n",
            "            print('intial: {}'.format(left_df.shape[0]+right_df.shape[0]))\n",
            "        if left=='WOS' or  re.search('^WOS_',left):\n",
            "            left_DOI='DI'\n",
            "            left_TI='TI'\n",
            "            left_extra_journal='SO' #helps with similiraty search  by Title\n",
            "        #elif\n",
            "        #else:\n",
            "            #sys.error('not supported left type')\n",
            "        #clean Tipo\n",
            "        if 'Tipo' in right_df:\n",
            "            right_df=right_df.drop('Tipo',axis='columns')\n",
            "            \n",
            "        if right=='SCI':\n",
            "            right_DOI='SCI_DI'\n",
            "            right_TI='SCI_TI'\n",
            "            right_extra_journal='SCI_SO' #helps with similiraty search  by Title\n",
            "            right_author='SCI_AU'\n",
            "            right_year='SCI_PY'\n",
            "        elif right=='SCP':\n",
            "            if 'SCP_Title' in right_df and not 'SCP_Title_0' in right_df:\n",
            "                right_df=split_translated_columns(right_df.copy(),on='SCP_Title',sep='\\[',min_title=16)\n",
            "            right_DOI='SCP_DOI'\n",
            "            right_TI='SCP_Title'\n",
            "            right_extra_journal='SCP_Source title' #helps with similiraty search  by Title\n",
            "            right_author='SCP_Authors'\n",
            "            right_year='SCP_Year'\n",
            "\n",
            "        #else:\n",
            "            #sys.error('not supported right type')\n",
            "\n",
            "        # Merge on DOIs                                 \n",
            "            \n",
            "        LEFT_RIGHT_inner=pd.DataFrame() \n",
            "        RIGHT_on=right_DOI\n",
            "        # RIGHT: no empty values for column 'right_df'\n",
            "        RIGHT,next_RIGHT=df_split(right_df,on=RIGHT_on,on_not_condition=True)\n",
            "        #full_RIGHT=RIGHT.append(next_RIGHT)\n",
            "        LEFT=left_df\n",
            "        LEFT_on=left_DOI\n",
            "        LEFT_series=clean(LEFT[LEFT_on])\n",
            "        RIGHT_series=clean(RIGHT[RIGHT_on])\n",
            "\n",
            "        LR=merge_inner_interior_exterior(LEFT.copy(),RIGHT.copy(),\\\n",
            "                    on_condition=RIGHT_on,left_on='LEFT_simple_doi',right_on='RIGHT_simple_doi',\\\n",
            "                                   left_series=LEFT_series,right_series=RIGHT_series)\n",
            "        if LR[0].shape[0]:\n",
            "            inner,new_LEFT,new_RIGHT=LR # LEFT.shape[0]=inner.shape[0]+new_LEFT.shape[0]\n",
            "                                        # RIGHT.shape[0]=inner.shape[0]+new_RIGHT.shape[0]\n",
            "            inner['Tipo']=inner['Tipo']+'_{}'.format(right)        \n",
            "            LEFT_RIGHT_inner=LEFT_RIGHT_inner.append(inner).reset_index(drop=True)\n",
            "        else:\n",
            "            new_LEFT=LEFT; new_RIGHT= RIGHT\n",
            "            \n",
            "\n",
            "        if DEBUG:\n",
            "            print(inner.shape[0],new_LEFT.shape[0],new_RIGHT.shape[0],'=,...,=')\n",
            "            print(LEFT.shape,RIGHT.shape)\n",
            "            print(new_LEFT.shape,(next_RIGHT.append(new_RIGHT)).shape,LEFT_RIGHT_inner.shape)\n",
            "            print( ( ( new_LEFT.append(LEFT_RIGHT_inner)  ).append(new_RIGHT) ).append(next_RIGHT).shape )\n",
            "            \n",
            "        #Merge on (splitted) Titles: generated with 'split_translated_columns' before\n",
            "        #next_RIGHT have column information even if empty\n",
            "        for nTI in [right_TI]+[ x for x in next_RIGHT.columns\n",
            "                                if re.search( '{}_[0-9]+'.format(right_TI),x  )]:\n",
            "            RIGHT_on=nTI\n",
            "            full_RIGHT=new_RIGHT.append(next_RIGHT)\n",
            "            RIGHT,next_RIGHT=df_split(full_RIGHT,on=RIGHT_on,on_not_condition=True)\n",
            "            RIGHT=RIGHT.drop_duplicates(RIGHT_on)\n",
            "            RIGHT_series=clean(RIGHT[RIGHT_on])\n",
            "\n",
            "            LEFT=new_LEFT\n",
            "            LEFT_series=clean(LEFT[left_TI])\n",
            "\n",
            "            LR=merge_inner_interior_exterior(LEFT.copy(),RIGHT.copy(),\\\n",
            "                                on_condition=RIGHT_on,left_on='LEFT_Simple_title',right_on='RIGHT_Simple_title',\\\n",
            "                                  left_series=LEFT_series,right_series=RIGHT_series)\n",
            "            if LR[0].shape[0]:\n",
            "                inner,new_LEFT,new_RIGHT=LR\n",
            "                inner['Tipo']=inner['Tipo']+'_{}'.format(right)\n",
            "                LEFT_RIGHT_inner=LEFT_RIGHT_inner.append(inner).reset_index(drop=True)\n",
            "            else:\n",
            "                new_LEFT=LEFT; new_RIGHT= RIGHT\n",
            "\n",
            "                \n",
            "            if DEBUG:\n",
            "                print(inner.shape[0],new_LEFT.shape[0],new_RIGHT.shape[0])\n",
            "                print(LEFT.shape,RIGHT.shape)\n",
            "                print(new_LEFT.shape,(next_RIGHT.append(new_RIGHT)).shape,LEFT_RIGHT_inner.shape) \n",
            "                print( ( ( new_LEFT.append(LEFT_RIGHT_inner)  ).append(new_RIGHT) ).append(next_RIGHT).shape )\n",
            "        \n",
            "        # Merge on Similar Titles\n",
            "        for nTI in [right_TI]+[ x for x in next_RIGHT.columns \n",
            "                                if re.search( '{}_[0-9]+'.format(right_TI),x  )]:                \n",
            "            RIGHT_on=nTI\n",
            "            full_RIGHT=new_RIGHT.append(next_RIGHT)\n",
            "            RIGHT,next_RIGHT=df_split(full_RIGHT,on=RIGHT_on,on_not_condition=True)\n",
            "            RIGHT=RIGHT.drop_duplicates(RIGHT_on)\n",
            "            RIGHT_series=clean(RIGHT[RIGHT_on]).str.replace('\\[','').str.replace('\\]','')\n",
            "\n",
            "            LEFT=new_LEFT\n",
            "            LEFT_series=clean(LEFT[left_TI])\n",
            "\n",
            "            LR=merge_inner_interior_exterior(LEFT.copy(),RIGHT.copy(),\\\n",
            "                                on_condition=RIGHT_on,left_on='LEFT_Simple_title',right_on='RIGHT_Simple_title',\\\n",
            "                                  left_series=LEFT_series,right_series=RIGHT_series,\\\n",
            "                                  left_extra_on=left_extra_journal,right_extra_on=right_extra_journal,\\\n",
            "                                  close_matches=True,cutoff=0.6)\n",
            "            if LR[0].shape[0]:\n",
            "                inner,new_LEFT,new_RIGHT=LR\n",
            "                inner['Tipo']=inner['Tipo']+'_{}'.format(right)\n",
            "                LEFT_RIGHT_inner=LEFT_RIGHT_inner.append(inner).reset_index(drop=True)\n",
            "            else:\n",
            "                new_LEFT=LEFT; new_RIGHT= RIGHT\n",
            "\n",
            "\n",
            "            if DEBUG:\n",
            "                print(inner.shape[0],new_LEFT.shape[0],new_RIGHT.shape[0])\n",
            "                print(LEFT.shape,RIGHT.shape)\n",
            "                print(new_LEFT.shape,(next_RIGHT.append(new_RIGHT)).shape,LEFT_RIGHT_inner.shape) \n",
            "                print( ( ( new_LEFT.append(LEFT_RIGHT_inner)  ).append(new_RIGHT) ).append(next_RIGHT).shape )\n",
            "                \n",
            "        \n",
            "        \n",
            "        full_RIGHT=next_RIGHT.append(new_RIGHT)\n",
            "        full_RIGHT['Tipo']=right\n",
            "        full_RIGHT['DI']=full_RIGHT[right_DOI]\n",
            "        full_RIGHT['TI']=full_RIGHT[right_TI]\n",
            "        full_RIGHT['SO']=full_RIGHT[right_extra_journal]\n",
            "        full_RIGHT['AU']=full_RIGHT[right_author]\n",
            "        full_RIGHT['PY']=full_RIGHT[right_year]\n",
            "\n",
            "        LEFT_RIGHT=new_LEFT\n",
            "        LEFT_RIGHT=LEFT_RIGHT.append(LEFT_RIGHT_inner)\n",
            "        LEFT_RIGHT=LEFT_RIGHT.append(full_RIGHT)\n",
            "        LEFT_RIGHT=fill_NaN(LEFT_RIGHT).reset_index(drop=True)\n",
            "         \n",
            "        if DEBUG:    \n",
            "            self.LEFT=LEFT\n",
            "            self.RIGHT=RIGHT\n",
            "            self.new_LEFT=new_LEFT\n",
            "            self.new_RIGHT=new_RIGHT\n",
            "            self.LEFT_RIGHT_inner=LEFT_RIGHT_inner\n",
            "            self.full_RIGHT=full_RIGHT\n",
            "        \n",
            "        exec('self.{}_{}=LEFT_RIGHT'.format(left,right))\n",
            "        self.type['{}_{}'.format(left,right)]='{}_{}'.format(left,right)\n",
            "        self.biblio['{}_{}'.format(left,right)]=LEFT_RIGHT\n",
            "\n",
            "if __name__=='__main__':\n",
            "    WOS_file='CIB_Wos.xlsx'\n",
            "    SCI_file='CIB_Scielo.xlsx'\n",
            "    SCP_file='CIB_Scopus.csv'\n",
            "    cib=wosplus('drive.cfg')\n",
            "    cib.load_biblio(WOS_file)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "T4Rmd2dF7JUQ",
        "colab_type": "code",
        "outputId": "a21d1a1d-d109-44ae-ed38-0d5e638f9ce9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "%%writefile drive.cfg\n",
        "[FILES]\n",
        "UDEA_WOS.xlsx       = 1px2IcrjCrkyu7t78Q7PAE5nzV_yuPt9t\n",
        "UDEA_SCI.xlsx       = 1pWMY5P72j0Ca6D-cm7dn7Q4TBGTs4PWV\n",
        "UDEA_SCP.xlsx       = 1ulCsFHzDiTmuL9TH8F58ulh0u8Z2ylKh\n",
        "UDEA_WOS_SCI_SCP.xlsx   = 1o9otmklgh-0w18Avv2ZTKOXr3vZbjwvj"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting drive.cfg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Y0D0hEdAMXUX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##  Load data bases"
      ]
    },
    {
      "metadata": {
        "id": "aonEaUgL5NAo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 451
        },
        "outputId": "bffdf95c-5913-44c8-9e00-69e902d6d21b"
      },
      "cell_type": "code",
      "source": [
        "UDEA=wp.wosplus('drive.cfg')\n",
        "tmp=UDEA.load_biblio('UDEA_WOS.xlsx') # creates WOS.WOS"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-160b556f30ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mUDEA\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwosplus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'drive.cfg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtmp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mUDEA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_biblio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'UDEA_WOS.xlsx'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# creates WOS.WOS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/wosplus/wosplus.py\u001b[0m in \u001b[0;36mload_biblio\u001b[0;34m(self, WOS_file, prefix)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;31m# Without prefix columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mWOS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tipo'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m             \u001b[0mWOS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Tipo'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         raise ValueError(\"The truth value of a {0} is ambiguous. \"\n\u001b[1;32m   1120\u001b[0m                          \u001b[0;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                          .format(self.__class__.__name__))\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0m__bool__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "occzrIeCS7aQ",
        "colab_type": "code",
        "outputId": "8511e159-02a4-4120-e883-b7f4e28675e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "UDEA=wp.wosplus('drive.cfg')\n",
        "tmp=UDEA.load_biblio('UDEA_WOS.xlsx') # creates WOS.WOS\n",
        "tmp=UDEA.load_biblio('UDEA_SCI.xlsx',prefix='SCI') # creates UDEA.SCI\n",
        "tmp=UDEA.load_biblio('UDEA_SCP.xlsx',prefix='SCP') # creates UDEA.SCP"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/wosplus/_wos_scp.py:65: FutureWarning: Using 'rename_axis' to alter labels is deprecated. Use '.rename' instead\n",
            "  for key in df.columns.values), axis=1)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "lnS28xGmuQp5",
        "colab_type": "code",
        "outputId": "736cc524-db1c-41ad-9a52-8b71f5d8fb99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print('before merge: {}'.format( UDEA.WOS.shape[0]+UDEA.SCI.shape[0]+UDEA.SCP.shape[0] )  )\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before merge: 24676\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3yKWoYAMdm0J",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### WOS+SCI"
      ]
    },
    {
      "metadata": {
        "id": "zsCnv55UuyNi",
        "colab_type": "code",
        "outputId": "64c6a2a5-4834-45f9-af5b-4bce0e6c15ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "UDEA.merge(left='WOS',right='SCI')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "................................................................................"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rDuqS3Muds3R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### WOS+SCI+SCP"
      ]
    },
    {
      "metadata": {
        "id": "XbpNjT40u83f",
        "colab_type": "code",
        "outputId": "f7a0c3f2-448f-456e-f1e2-dc70c8dc9a6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "UDEA.merge(left='WOS_SCI',right='SCP')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "................................................................................................................................................................................................................"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f5eF0y50vLrm",
        "colab_type": "code",
        "outputId": "3c02700b-df3a-406e-adf8-06d11039ff01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print( 'after merge: {}'.format(UDEA.WOS_SCI_SCP.shape[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "after merge: 15504\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "R503IhFiaFbA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "UDEA.WOS_SCI_SCP.to_excel('UDEA_WOS_SCI_SCP.xlsx',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xlb-50u2a3ib",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "See the resulting Excel file by opening the side bar → Files → REFRESH and right click for Download upon the generated  \"'UDEA_WOS_SCI_SCP.xlsx'\" fille, or just check the corresponsing DataFrame:"
      ]
    },
    {
      "metadata": {
        "id": "CN8Rl9rdfASF",
        "colab_type": "code",
        "outputId": "fe0660cc-a971-41a8-8e35-0d2f74eef5ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "cell_type": "code",
      "source": [
        "UDEA.WOS_SCI_SCP.sample()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>AB</th>\n",
              "      <th>AF</th>\n",
              "      <th>AR</th>\n",
              "      <th>AU</th>\n",
              "      <th>BA</th>\n",
              "      <th>BE</th>\n",
              "      <th>BF</th>\n",
              "      <th>BN</th>\n",
              "      <th>BP</th>\n",
              "      <th>C1</th>\n",
              "      <th>...</th>\n",
              "      <th>SU</th>\n",
              "      <th>TC</th>\n",
              "      <th>TI</th>\n",
              "      <th>Tipo</th>\n",
              "      <th>U1</th>\n",
              "      <th>U2</th>\n",
              "      <th>UT</th>\n",
              "      <th>VL</th>\n",
              "      <th>WC</th>\n",
              "      <th>Z9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td></td>\n",
              "      <td>Vasquez, E.\\nFranco, J. L.\\nOrrego, J. C.\\n</td>\n",
              "      <td></td>\n",
              "      <td>Vasquez, E\\nFranco, JL\\nOrrego, JC\\n</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>S126\\n</td>\n",
              "      <td>[Vasquez, E.; Franco, J. L.; Orrego, J. C.] Un...</td>\n",
              "      <td>...</td>\n",
              "      <td>3\\n</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Subcutaneous (SCIg) Gammaglobulin for the Trea...</td>\n",
              "      <td>WOS</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>WOS:000347388100064\\n</td>\n",
              "      <td>33\\n</td>\n",
              "      <td>Immunology\\n</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 154 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    AB                                           AF AR  \\\n",
              "568     Vasquez, E.\\nFranco, J. L.\\nOrrego, J. C.\\n      \n",
              "\n",
              "                                       AU BA BE BF BN      BP  \\\n",
              "568  Vasquez, E\\nFranco, JL\\nOrrego, JC\\n              S126\\n   \n",
              "\n",
              "                                                    C1 ...    SU   TC  \\\n",
              "568  [Vasquez, E.; Franco, J. L.; Orrego, J. C.] Un... ...   3\\n  0.0   \n",
              "\n",
              "                                                    TI Tipo   U1   U2  \\\n",
              "568  Subcutaneous (SCIg) Gammaglobulin for the Trea...  WOS  0.0  0.0   \n",
              "\n",
              "                        UT    VL            WC   Z9  \n",
              "568  WOS:000347388100064\\n  33\\n  Immunology\\n  0.0  \n",
              "\n",
              "[1 rows x 154 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "ju7yZPvOrT8V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xbtsF0Vn5IeJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load merged database"
      ]
    },
    {
      "metadata": {
        "id": "0p_GDpIe5NNt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "udea=wp.wosplus('drive.cfg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PAB08BZa5b-8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#udea.load_biblio('UDEA_WOS_SCI_SCP.xlsx')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dzTWh46t5dLa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "UDEA=udea.read_drive_excel('UDEA_WOS_SCI_SCP.xlsx')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RwPiIZyf6ci0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "UDEA=UDEA.fillna('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZWK5wJdC7sVj",
        "colab_type": "code",
        "outputId": "3b8c6f20-d419-48e0-d226-e38405f89765",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "wp.df_split( UDEA, 'DI',Operator='!=',condition='')[0].shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7993, 154)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "yoGRnRCr6s8R",
        "colab_type": "code",
        "outputId": "74f97b16-4bc1-48f4-e025-932247d216da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "for t in ['WOS','SCI','SCP']:\n",
        "    print( 'DOIS {}: {}'.format(t,\n",
        "        wp.df_split( UDEA[ UDEA.Tipo.str.contains(t) ], 'DI',Operator='!=',condition='')[0].shape) )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DOIS WOS: (6025, 154)\n",
            "DOIS SCI: (1409, 154)\n",
            "DOIS SCP: (6955, 154)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JsNaXsty60k4",
        "colab_type": "code",
        "outputId": "b8630d80-6660-46b8-f12b-d9406a90765a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6025, 154)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "feV_64CK7VEp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}